{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "muslim-monte",
   "metadata": {},
   "source": [
    "#  Обучаем Word2Wec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-spider",
   "metadata": {},
   "source": [
    "## Подготовка dataset-а"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "similar-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_parquet('data_fusion_train.parquet', engine='pyarrow')\n",
    "train = train[train.category_id == -1].drop_duplicates('item_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_names = train['item_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "restricted-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/web/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "patterns = \"[A-Za-z0-9!#$%&№'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token)\n",
    "    if len(tokens) > 2:\n",
    "        return tokens\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-adams",
   "metadata": {},
   "source": [
    "## Обрабатываем 1/10 от всех данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1571/3107 [37:10<36:54,  1.44s/it] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "result = []\n",
    "num_iters = int(len(product_names)/1000)\n",
    "last_iter = 0\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_iters)):\n",
    "    i+=1\n",
    "    if i == num_iters:\n",
    "        result+=product_names[num_iters*1000:].apply(lemmatize).tolist()\n",
    "    else:\n",
    "        result+=product_names[last_iter*1000:i*1000].apply(lemmatize).tolist()\n",
    "    last_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[_ for _ in ws if len(_) > 2] for ws in result if ws is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "for tokens in result:\n",
    "    for token in tokens:\n",
    "        word_freq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-worse",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=2,\n",
    "    size=300,\n",
    "    negative=10,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"хлеб\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-equipment",
   "metadata": {},
   "source": [
    "## Проверка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet('data_fusion_train.parquet', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.category_id != -1].drop_duplicates('item_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_names = test['item_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = product_names.apply(lemmatize).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_arrs = [[_ for _ in ws if len(_) > 2] if ws is not None else [] for ws in test_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "embendding_len = len(w2v_model.wv.word_vec(\"хлеб\"))\n",
    "embendding_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embenddings = []\n",
    "for words in words_arrs:\n",
    "    element_embenddings = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            element_embenddings.append(w2v_model.wv.word_vec(word))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(element_embenddings) == 0:\n",
    "        test_embenddings.append([0]*embendding_len)\n",
    "    else:\n",
    "        test_embenddings.append(sum(element_embenddings))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_embenddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_embenddings\n",
    "labels = test[\"category_id\"].tolist()\n",
    "print(len(data))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-teddy",
   "metadata": {},
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(classifier.predict(data), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-fitting",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components = 2, init = 'pca', random_state = 0)\n",
    "data_2d_tsne = tsne.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as PLT\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.figure(figsize = (10, 6))\n",
    "pylab.scatter(data_2d_tsne[:, 0], data_2d_tsne[:, 1], c = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-acquisition",
   "metadata": {},
   "source": [
    "## Количество нулевых векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for embendding in tqdm(data):\n",
    "    if np.array_equal(embendding, [0]*300):\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count*100/len(data), \"% элементов выборки не обрабатываются моделью\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-python",
   "metadata": {},
   "source": [
    "## Примеры необрабатываемых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for embendding, words_arr, product_name in zip(data, words_arrs, product_names.tolist()):\n",
    "    if np.array_equal(embendding, [0]*300):\n",
    "        print(product_name)\n",
    "#         print(words_arr)\n",
    "        count += 1\n",
    "    if count >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-window",
   "metadata": {},
   "source": [
    "## Нужно обучать на всей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
